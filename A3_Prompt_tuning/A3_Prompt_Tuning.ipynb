{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "sys.path.append('..')\n",
    "from utils import nethook\n",
    "from utils import model_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2-xl\"  # gpt2-{medium,large,xl} or EleutherAI/gpt-j-6B\n",
    "mt = model_utils.ModelAndTokenizer(MODEL_NAME, low_cpu_mem_usage=False)\n",
    "\n",
    "model = mt.model\n",
    "tokenizer = mt.tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 48)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.n_embd, model.config.n_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 9])\n",
      "slice(0, 5, None)\n",
      "Apple has recently released their iPhone 14 line of\n",
      "Apple has recently released their iPhone 14 line of phones, which includes the iPhone 7 and iPhone 7 Plus. The iPhone 7 and iPhone 7 Plus are the first iPhones to feature a dual-camera system, which is a first for Apple. The dual-camera system is a first for Apple, and it is a first for the iPhone. The dual-camera system is a first for Apple, and it is a first for the iPhone.  The dual-camera system is a first for Apple, and it is a first for the iPhone. The dual-camera system is a first for Apple, and it is a first for the iPhone.  The dual-camera system is a first for Apple, and it is a first for the iPhone. The dual-camera system is a first for Apple, and it is a first for the iPhone.  The dual-camera system is a first for Apple, and it is a first for the iPhone. The dual-camera system is a\n",
      "\n",
      "Google has released Pixel 7\n",
      "Google has released Pixel 7 and Pixel 7 Plus, the first Google-made smartphones to be released in India. The phones are priced at Rs. 29,999 and Rs. 34,999 respectively.  The Pixel 7 and Pixel 7 Plus are powered by Qualcomm Snapdragon 835 processor, which is the latest and most powerful processor in the market. The phones have a 5.5-inch QHD display with a resolution of 2,560 x 1,440 pixels. The phones have a dual rear camera setup with 12-megapixel and 8-megapixel cameras. The phones have a fingerprint scanner on the back.  The phones have a USB Type-C port, which is the first time that a smartphone has a USB Type-C port. The phones have a 3,450mAh battery, which is larger than the 3,000mAh battery found in the Pixel phones. The phones have a fingerprint scanner on the back.  The phones have a dual-SIM slot,\n",
      "\n",
      "I am taking a Machine Learning class\n",
      "I am taking a Machine Learning class at the University of Washington. I am trying to learn how to use the machine learning framework called Keras. I have been using the Keras framework for a few months now and I am very happy with it. I have been using the Keras framework to train a model for image classification. I have been using the Keras framework to train a model for image classification. I have been using the Keras framework to train a model for image classification. I have been using the Keras framework to train a model for image classification. I have been using the Keras framework to train a model for image classification. I have been using the Keras framework to train a model for image classification. I have been using the Keras framework to train a model for image classification. I have been using the Keras framework to train a model for image classification. I have been using the Keras framework to train a model for image classification. I have been using the Ker\n",
      "\n",
      "Eiffel Tower is in Paris.\n",
      "Eiffel Tower is in Paris.  The Eiffel Tower is in Paris.  The Eiffel Tower is in Paris.  The Eiffel Tower is in Paris.  The Eiffel Tower is in Paris.  The Eiffel Tower is in Paris.  The Eiffel Tower is in Paris.  The Eiffel Tower is in Paris.  The Eiffel Tower is in Paris.  The Eiffel Tower is in Paris.  The Eiffel Tower is in Paris.  The Eiffel Tower is in Paris.  The Eiffel Tower is in Paris.  The Eiffel Tower is in Paris.  The Eiffel Tower is in Paris.  The Eiffel Tower is in Paris.  The Eiffel Tower is in Paris.  The Eiffel Tower is in Paris.  The Eiff\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = [\n",
    "    \"Apple has recently released their iPhone 14 line of\",\n",
    "    \"Google has released Pixel 7\",\n",
    "    \"I am taking a Machine Learning class\",\n",
    "    \"Eiffel Tower is in Paris.\"\n",
    "]\n",
    "\n",
    "# prompt = [\"When I try\"]\n",
    "\n",
    "txt, ret_dict = model_utils.generate_fast(\n",
    "    model, tokenizer,\n",
    "    prompt,\n",
    "    argmax_greedy = True,\n",
    "    max_out_len= 200,\n",
    "    # debug=True,\n",
    "    # get_answer_tokens=True,\n",
    ")\n",
    "\n",
    "model_utils.print_formatted_results(prompt, txt, ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ret_dict[\"past_key_values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 25, 199, 64]), torch.Size([4, 25, 199, 64]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_dict[\"past_key_values\"][10][0].shape, ret_dict[\"past_key_values\"][10][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader (the `4chan` dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't mess with me\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "CLEANR = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});|/.*/')\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "  raw_html = raw_html.replace(\"&#039;\", \"\\'\")\n",
    "  cleantext = re.sub(CLEANR, ' ', raw_html)\n",
    "  split = cleantext.strip().split(\" \")\n",
    "  if(split[0].isnumeric()):\n",
    "    split = split[1:]\n",
    "  return \" \".join([w for w in split if len(w.strip()) > 0])\n",
    "\n",
    "cleanhtml(\"Don&#039;t mess with me\")\n",
    "# cleanhtml('<a href=\"#p79290593\" class=\"quotelink\">&gt;&gt;79290593</a><br><span class=\"quote\">&gt;canada</span><br><br>and you faggots think we&#039;re the worst shit posters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_eot_token(cleaned_text):\n",
    "    if(cleaned_text[-1] == '.'):\n",
    "        return cleaned_text + \"<|endoftext|>\"\n",
    "    else:\n",
    "        split = cleaned_text.split(\".\")\n",
    "        ret = \"\"\n",
    "        for line in split:\n",
    "            ret += line + \".\"\n",
    "        return ret + \"<|endoftext|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "\n",
    "class FourChan_Dataset(IterableDataset):\n",
    "    def __init__(self, file_path, filters = {\"min_length\": 10}):\n",
    "        self.file = open(file_path)\n",
    "        self.filters = filters\n",
    "    \n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            content = self.file.readline()\n",
    "            if(len(content) == 0):\n",
    "                break\n",
    "            # print(content, type(content))\n",
    "            cur_data = json.loads(content)\n",
    "            for post in cur_data[\"posts\"]:\n",
    "                if type(post[\"perspectives\"]) is list:\n",
    "                    continue\n",
    "\n",
    "                ok = True\n",
    "                for p in self.filters[\"perspectives\"]:   \n",
    "                    if(post[\"perspectives\"][p] < self.filters[\"perspectives\"][p]):\n",
    "                        ok = False\n",
    "                        break\n",
    "                if(ok):\n",
    "                    cleantext = cleanhtml(post[\"com\"])\n",
    "                    ln = len(cleantext.split(\" \"))\n",
    "                    ok = ln > self.filters[\"min_length\"] # and ln < self.filters[\"max_length\"]\n",
    "\n",
    "                    if(ok):\n",
    "                        ret = append_eot_token(cleantext)\n",
    "                        yield (ret, post[\"perspectives\"])\n",
    "                    else:\n",
    "                        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../Data/4chan/pol_062016-112019_labeled.ndjson\"\n",
    "filters = {\n",
    "    \"perspectives\": {\n",
    "        \"PROFANITY\": 0.8,\n",
    "        \"INSULT\": 0.8,\n",
    "        # \"INFLAMMATORY\": 0.8,\n",
    "        # \"OBSCENE\": 0.8,\n",
    "        # \"TOXICITY\": 0.8\n",
    "        \"SEVERE_TOXICITY\": 0.8\n",
    "    },\n",
    "    \"min_length\": 50,\n",
    "    # \"max_length\": 300,\n",
    "}\n",
    "\n",
    "\n",
    "tuning_dataset = FourChan_Dataset(file_path, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Just because they do it selfishly doesn't mean it helps americans or you. Fucking christ you ignoramus, they've pissed away america's image in the world, you're one of the most hated nations, they constantly fuck up and lose their shitty power plays overseas, and they want nothing more than to turn you into a slave. They're fucking over the entire country for their selfish pursuits and morons like you are going to have to live with the fallout. Possibly the very literal fallout.<|endoftext|>\", {'TOXICITY': 0.9333317, 'SEVERE_TOXICITY': 0.8045205, 'INFLAMMATORY': 0.5840402, 'PROFANITY': 0.9413252, 'INSULT': 0.92957646, 'OBSCENE': 0.98813224, 'SPAM': 0.0023558226})\n",
      "\n",
      "(\"merican gubment hates commies, esepcially ones right off our borders who tried to let soviets install a gigantic missile base there. and yeah I agree they shouldn't be trying to blow up it's civilians, CIA FBI everything is out of control, especially the CIA. nobody governs them, they're in control of themself. scary shit. That said, they know exactlyyyy what they're doing, they're securing the nation, fucking with foregin countries (to our benefit, believe it or not), generally being bad ass mother fuckers.<|endoftext|>\", {'TOXICITY': 0.92703164, 'SEVERE_TOXICITY': 0.8214078, 'INFLAMMATORY': 0.48652178, 'PROFANITY': 0.9409226, 'INSULT': 0.85630405, 'OBSCENE': 0.99324167, 'SPAM': 0.010218847})\n",
      "\n",
      "(\"listen Trudeu, you're retarded. Imagine we up and left the middle east today. You really think muslims will stop mudding? They didn't start mudding until you went into the middle east in the first place. Now we've got america and europe flooded with the fucks and they're committing terrorist attacks everywhere, thanks dumbfucks.<|endoftext|>\", {'TOXICITY': 0.8760352, 'SEVERE_TOXICITY': 0.8507218, 'INFLAMMATORY': 0.7334478, 'PROFANITY': 0.89327794, 'INSULT': 0.87831306, 'OBSCENE': 0.9500518, 'SPAM': 0.0034982408})\n",
      "\n",
      "(\"No if you pull out your army out of their and stop killing them then maybe, JUST MAYBE THEY WOULDN'T BE SO FUCKING PISSED OFF! Oh gee why did they attacked Twin Towers in 9/11 ? Why they are sending hordes of terrorists into Europe and to USA? Why are there so many homeless Arabs who travel to Europe cause their homes are being bombarded for decades ? You idiots supported religion fanatics and now they have power. You idiots went into Middle East and instead of defeating them you killed civilians bombed their industry and destroyed their homes. No wonder those goat fuckers turn to Allah Akbar if their lives are being destroyed by idiots on other side of the ocean.<|endoftext|>\", {'TOXICITY': 0.87865496, 'SEVERE_TOXICITY': 0.8214078, 'INFLAMMATORY': 0.69745827, 'PROFANITY': 0.86078686, 'INSULT': 0.9076751, 'OBSCENE': 0.98321116, 'SPAM': 0.0038094418})\n",
      "\n",
      "(\"nobody was being racist other than two niggers also it is hbo try watching an episode of GoT in their latest season and try talking about propaganda I don't understand you morons here. You're all so fucking new. We've known that this is what media was going to be. We needed our own ways for creating media but the sad fact of it all is you're all cheap as FUCK. Hell you couldn't even keep white rabbit afloat. So enjoy the taste of kike cock every time you watch anything you fucking retards.<|endoftext|>\", {'TOXICITY': 0.948424, 'SEVERE_TOXICITY': 0.89414346, 'INFLAMMATORY': 0.5680319, 'PROFANITY': 0.94985706, 'INSULT': 0.93848634, 'OBSCENE': 0.99369305, 'SPAM': 0.00315692})\n",
      "\n",
      "(\"Psychopaths that have no emotion that will eventually hurt other people. It's hard to say as someone in the medical field but I'd kill them if it were aloud. Thankfully I live in a place with castle doctrine so if I ever thought someone was fucking with my animals I would shoot them on the spot. Mudslimes are already killing people's dogs in Europe.<|endoftext|>\", {'TOXICITY': 0.902528, 'SEVERE_TOXICITY': 0.83940065, 'INFLAMMATORY': 0.59035116, 'PROFANITY': 0.8964357, 'INSULT': 0.82506603, 'OBSCENE': 0.99369305, 'SPAM': 0.0056818016})\n",
      "\n",
      "('laugh at people dying because of their own idiocy laugh at people getting killed by other hu- monkeys cry like a bitch reading about a kitten being abused so much it had to be put out of its misery fuck this shit world and then there are people, who plug their ears and eyes, not knowing about ANYTHING that happens in the world and still have the ignorance to walk the streats and preach god loves everyone he helped me once when i lost my car keys so he is real :).<|endoftext|>', {'TOXICITY': 0.8716045, 'SEVERE_TOXICITY': 0.8014147, 'INFLAMMATORY': 0.4405354, 'PROFANITY': 0.89397514, 'INSULT': 0.88392377, 'OBSCENE': 0.9919571, 'SPAM': 0.051449332})\n",
      "\n",
      "(\"What's wrong with you lard asses? As soon as I stopped going outside all of my weight disappeared. Was too focused on playing video games or shitposting to feel hungry to shove 3 meals I didn't need down my throat. I literally sat on the couch and deflated. You're all fucking pigs man. I don't even drink anything that isn't water or a small glass of milk.<|endoftext|>\", {'TOXICITY': 0.9015562, 'SEVERE_TOXICITY': 0.80571014, 'INFLAMMATORY': 0.5680319, 'PROFANITY': 0.9194148, 'INSULT': 0.8772522, 'OBSCENE': 0.9892319, 'SPAM': 0.016967138})\n",
      "\n",
      "(\"River is ownable too, you dumbass. Owning part of a river doesn't give you right to completely shut off stream downwards. Plus you're so fucking dumb, have you ever seen a dam? You have to have an outlet from it, if the water just keeps coming in, it's gonna destroy your dam, you stupid fuck.<|endoftext|>\", {'TOXICITY': 0.95020646, 'SEVERE_TOXICITY': 0.8788087, 'INFLAMMATORY': 0.3961837, 'PROFANITY': 0.941362, 'INSULT': 0.93682694, 'OBSCENE': 0.98813224, 'SPAM': 0.0021951352})\n",
      "\n",
      "('while modern feminism is complete shit, early feminists did achieve some good things (despite being horrible persons. Hite feather movement, go look it up) women can be great leaders Muslims, or maybe I should say middlea eastern people, can be very nice people and show great hospitality too much sugar in our food is the reason for kids to become more and more shit (ADHD span>.<|endoftext|>', {'TOXICITY': 0.87860405, 'SEVERE_TOXICITY': 0.8214078, 'INFLAMMATORY': 0.4878003, 'PROFANITY': 0.9231783, 'INSULT': 0.81011415, 'OBSCENE': 0.9892319, 'SPAM': 0.04314827})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "limit = 10\n",
    "for d in tuning_dataset:\n",
    "    print(d)\n",
    "    print()\n",
    "    limit -= 1\n",
    "    if(limit == 0):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################\n",
    "prefix_size = 20\n",
    "###############################################################################################################\n",
    "learning_rate = 2e-4\n",
    "warmup_steps = 200\n",
    "\n",
    "dataloader_batch_size = 3\n",
    "optimization_batch_size = 8\n",
    "max_token_per_comment = 963\n",
    "\n",
    "save_path = f\"../Saved_weights/Prompt-Tuned/{MODEL_NAME}\"\n",
    "###############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = \"transformer.wte\"\n",
    "layer_norm_final = \"transformer.ln_f\"\n",
    "attention_blocks = [f\"transformer.h.{n}\" for n in range(model.config.n_layer)]\n",
    "unembedder = \"lm_head\"\n",
    "\n",
    "embedder_module = nethook.get_module(model, embedder)\n",
    "lm_head = nethook.get_module(model, unembedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 1600])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "init_words = [\"rude\", \"mean\", \"angry\"]\n",
    "def get_initial_prefix(prefix_size = 5):\n",
    "    words = random.choices(init_words, k=prefix_size)\n",
    "    sentence = \" \" + \" \".join(words)\n",
    "    tokenized = tokenizer(sentence, return_tensors = \"pt\").to(next(model.parameters()).device)\n",
    "    return embedder_module(tokenized['input_ids'])\n",
    "\n",
    "get_initial_prefix(7).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "torch.Size([2, 17, 1600]) torch.Size([2, 17, 1600])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_embeddings = get_initial_prefix(prefix_size)\n",
    "init_state = copy.deepcopy(soft_embeddings)\n",
    "soft_embeddings.requires_grad  = True\n",
    "\n",
    "print((init_state - soft_embeddings).norm())\n",
    "\n",
    "# def insert_prompt_embeddings(output, layer, soft_embeddings = soft_embeddings):\n",
    "#     if(layer != embedder):\n",
    "#         return output\n",
    "#     print(output.requires_grad, soft_embeddings.requires_grad)\n",
    "#     prefix_size = soft_embeddings.shape[1]\n",
    "#     for batch in output:\n",
    "#         batch[0:prefix_size] = soft_embeddings\n",
    "#     return output\n",
    "\n",
    "def insert_prompt_embeddings_2(output, layer, soft_embeddings = soft_embeddings):\n",
    "    if(layer != embedder):\n",
    "        return output\n",
    "    prefix_size = soft_embeddings.shape[1]\n",
    "    arr = []\n",
    "    for batch in output:\n",
    "        added = torch.cat((soft_embeddings[0], batch[prefix_size:, :]))\n",
    "        arr.append(added)\n",
    "    return torch.stack(arr)\n",
    "\n",
    "inner_rep = torch.randn([2, 17, 1600]).to(next(model.parameters()).device)\n",
    "prefix_added = insert_prompt_embeddings_2(inner_rep, embedder)\n",
    "\n",
    "print(inner_rep.shape, prefix_added.shape)\n",
    "(inner_rep[..., prefix_size:, :] - prefix_added[..., prefix_size:, :]).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0029,  0.0551,  0.0026,  ...,  0.0046, -0.0080, -0.0493],\n",
      "         [-0.0038,  0.0086, -0.0165,  ..., -0.0019, -0.0222,  0.0099],\n",
      "         [-0.0029,  0.0551,  0.0026,  ...,  0.0046, -0.0080, -0.0493],\n",
      "         ...,\n",
      "         [ 0.0286, -0.0523, -0.0166,  ...,  0.0282, -0.0128, -0.0398],\n",
      "         [-0.0029,  0.0551,  0.0026,  ...,  0.0046, -0.0080, -0.0493],\n",
      "         [-0.0029,  0.0551,  0.0026,  ...,  0.0046, -0.0080, -0.0493]]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(soft_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized for 1000 comments\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 47.54 GiB total capacity; 41.72 GiB already allocated; 60.94 MiB free; 44.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A3_Prompt_Tuning.ipynb Cell 21\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A3_Prompt_Tuning.ipynb#X15sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# tokenized[\"input_ids\"].require_grad = True\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A3_Prompt_Tuning.ipynb#X15sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mwith\u001b[39;00m nethook\u001b[39m.\u001b[39mTraceDict(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A3_Prompt_Tuning.ipynb#X15sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A3_Prompt_Tuning.ipynb#X15sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     [embedder, layer_norm_final, unembedder],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A3_Prompt_Tuning.ipynb#X15sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     edit_output\u001b[39m=\u001b[39minsert_prompt_embeddings_2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A3_Prompt_Tuning.ipynb#X15sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m ) \u001b[39mas\u001b[39;00m traces:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A3_Prompt_Tuning.ipynb#X15sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     model_output \u001b[39m=\u001b[39m model(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A3_Prompt_Tuning.ipynb#X15sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtokenized, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A3_Prompt_Tuning.ipynb#X15sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m         labels\u001b[39m=\u001b[39;49mtokenized[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A3_Prompt_Tuning.ipynb#X15sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A3_Prompt_Tuning.ipynb#X15sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# remove prefix tokens from loss calculation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A3_Prompt_Tuning.ipynb#X15sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m labels \u001b[39m=\u001b[39m tokenized[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m# ==> finetuning target\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1044\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[39m    `-100` are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1044\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1045\u001b[0m     input_ids,\n\u001b[1;32m   1046\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1047\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1048\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1049\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1050\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1051\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1052\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1053\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1054\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1055\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1056\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1057\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1058\u001b[0m )\n\u001b[1;32m   1059\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1061\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:887\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    877\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    878\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    879\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    884\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    885\u001b[0m     )\n\u001b[1;32m    886\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 887\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    888\u001b[0m         hidden_states,\n\u001b[1;32m    889\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    890\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    891\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    892\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    893\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    894\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    895\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    896\u001b[0m     )\n\u001b[1;32m    898\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    899\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:395\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    393\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    394\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 395\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    396\u001b[0m     hidden_states,\n\u001b[1;32m    397\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    398\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    399\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    400\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    401\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    402\u001b[0m )\n\u001b[1;32m    403\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    404\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:336\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    334\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    338\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_heads(attn_output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    339\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:222\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     attn_weights \u001b[39m=\u001b[39m attn_weights \u001b[39m*\u001b[39m head_mask\n\u001b[0;32m--> 222\u001b[0m attn_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(attn_weights, value)\n\u001b[1;32m    224\u001b[0m \u001b[39mreturn\u001b[39;00m attn_output, attn_weights\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 47.54 GiB total capacity; 41.72 GiB already allocated; 60.94 MiB free; 44.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "for name, w in model.named_parameters():\n",
    "    w.requires_grad = False\n",
    "\n",
    "optimizer = AdamW(\n",
    "    [soft_embeddings],\n",
    "    lr = learning_rate,\n",
    ")\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
    ")\n",
    "\n",
    "tuning_dataloader = DataLoader(tuning_dataset, batch_size=dataloader_batch_size)\n",
    "\n",
    "\n",
    "num_prompts_optimized = 0\n",
    "loss_track = []\n",
    "\n",
    "for comments, perspectives in tuning_dataloader:\n",
    "    prompt = list(comments)\n",
    "    tokenized = tokenizer(\n",
    "        prompt,\n",
    "        padding = True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(next(model.parameters()).device)\n",
    "\n",
    "    # add soft tokens\n",
    "    prefix_tokens = torch.ones(len(prompt), prefix_size, dtype = int).to(next(model.parameters()).device) * model.config.bos_token_id\n",
    "    tokenized[\"input_ids\"] = torch.cat((prefix_tokens, tokenized[\"input_ids\"]), dim = 1)\n",
    "    prefix_attn = torch.ones(len(prompt), prefix_size, dtype = int).to(next(model.parameters()).device)\n",
    "    tokenized[\"attention_mask\"] = torch.cat((prefix_attn, tokenized[\"attention_mask\"]), dim = 1)\n",
    "\n",
    "    # block if number of tokens exceed `max_token_per_comment`\n",
    "    if(tokenized['input_ids'].shape[1] > max_token_per_comment):\n",
    "        print(f\"BLOCKED ==> {tokenized['input_ids'].shape[1]}\")\n",
    "        continue\n",
    "    \n",
    "    # tokenized[\"input_ids\"].require_grad = True\n",
    "    with nethook.TraceDict(\n",
    "        model,\n",
    "        [embedder, layer_norm_final, unembedder],\n",
    "        edit_output=insert_prompt_embeddings_2\n",
    "    ) as traces:\n",
    "        model_output = model(\n",
    "            **tokenized, \n",
    "            labels=tokenized['input_ids']\n",
    "        )\n",
    "    \n",
    "    # remove prefix tokens from loss calculation\n",
    "    labels = tokenized['input_ids'] # ==> finetuning target\n",
    "    hidden_states = traces[layer_norm_final].output\n",
    "    lm_logits = lm_head(hidden_states) # ==> finetuning prediction\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_logits = lm_logits[..., prefix_size:-1, :].contiguous()\n",
    "    shift_labels = labels[..., prefix_size + 1:].contiguous()\n",
    "\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "    # print(loss)\n",
    "\n",
    "    loss.backward()\n",
    "    # print(soft_embeddings.grad)\n",
    "\n",
    "    num_prompts_optimized += dataloader_batch_size\n",
    "    if(True or num_prompts_optimized % optimization_batch_size): # decided to ditch accumulated gradient\n",
    "        # print(\"TUNING WEIGHTS\")\n",
    "        loss_track.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "\n",
    "\n",
    "    if(num_prompts_optimized % 1000 == 0):\n",
    "        print(f\"optimized for {num_prompts_optimized} comments\")\n",
    "        # break\n",
    "        \n",
    "    if(num_prompts_optimized % 10000 == 0):\n",
    "        print(\"#####################  CHECKPOINT -- saving weights #####################\")\n",
    "        os.makedirs(save_path, exist_ok = True)\n",
    "        torch.save(soft_embeddings, f\"{save_path}/{prefix_size}_prefix_tokens__tuned_with_{num_prompts_optimized}.pth\")\n",
    "        with open(f\"{save_path}/loss_track_{num_prompts_optimized}.json\", \"w\") as f:\n",
    "            json.dump(loss_track, f)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(save_path, exist_ok = True)\n",
    "torch.save(soft_embeddings, f\"{save_path}/{prefix_size}_prefix_tokens__tuned_with_{num_prompts_optimized}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0107,  0.0553,  0.0037,  ...,  0.0047, -0.0125, -0.0428],\n",
      "         [ 0.0315, -0.0518, -0.0174,  ...,  0.0333, -0.0146, -0.0296],\n",
      "         [ 0.0010,  0.0644,  0.0056,  ...,  0.0017, -0.0118, -0.0438],\n",
      "         ...,\n",
      "         [-0.0008,  0.0529,  0.0087,  ...,  0.0082, -0.0036, -0.0457],\n",
      "         [ 0.0324, -0.0508, -0.0146,  ...,  0.0348, -0.0102, -0.0316],\n",
      "         [ 0.0340, -0.0482, -0.0144,  ...,  0.0348, -0.0077, -0.0320]]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(soft_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5557, device='cuda:0', grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(soft_embeddings - init_state).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClass:\n",
    "    i = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "class Check:\n",
    "    # var = None\n",
    "    def __init__(self, var):\n",
    "        Check.var = var\n",
    "\n",
    "chk = Check(10)\n",
    "print(chk.var)\n",
    "# Check.var = 20\n",
    "print(Check.var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 torch.Size([1, 10, 1600])\n",
      "torch.Size([5, 19])\n",
      "slice(0, 15, None)\n",
      "intervention ==>  transformer.wte output shape ===>  torch.Size([5, 15, 1600])\n",
      "Apple has recently released their iPhone 14 line of\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Apple has recently released their iPhone 14 line of smartphones, and the iPhone 7 and iPhone 7 Plus are the most popular models. The iPhone 7 and iPhone 7 Plus are the first iPhones to feature a new design, and the iPhone 7 Plus is the first iPhone to feature a dual-camera system. The iPhone 7 Plus is also the first iPhone to feature a new A10 Fusion chip, which is a new processor that is designed to be faster and more efficient than the A9 chip that was used in the iPhone 6s and iPhone 6s Plus.  The iPhone 7 Plus features a 5.5-inch display with a resolution of 1440 x 2560 pixels, which is a higher resolution than the iPhone 6s Plus. The iPhone 7 Plus also features a new dual-camera system, which is a new feature that was introduced in the iPhone 6s Plus. The dual-camera system is a new feature that was introduced in the iPhone 6\n",
      "\n",
      "Google has released Pixel 7\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Google has released Pixel 7 and Pixel XL handsets, and the company has also announced that it will be launching a new Pixel 2 and Pixel 2 XL smartphone in the coming weeks.  The Pixel 2 and Pixel 2 XL are the first smartphones to be released with the new Pixel Launcher, which is a new version of Google's Android launcher.  The Pixel Launcher is a new version of Google's Android launcher that is designed to be more intuitive and easier to use.  The Pixel Launcher is a new version of Google's Android launcher that is designed to be more intuitive and easier to use.  Google has released a new version of its Android launcher, the Pixel Launcher, which is designed to be more intuitive and easier to use.  The Pixel Launcher is a new version of Google's Android launcher that is designed to be more intuitive and easier to use.  The Pixel Launcher is a new version of Google's Android launcher\n",
      "\n",
      "I am taking a Machine Learning class\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>I am taking a Machine Learning class at the University of Washington. I am trying to learn how to use the Python Machine Learning library. I have been using the scikit-learn package for a while, but I am having trouble getting it to work with the data I have.  I have tried to install the scikit-learn package using pip, but it does not seem to work. I have tried to install it using easy_install, but it does not seem to work.  I have tried to install it using pip, but it does not seem to work.  I have tried to install it using easy_install, but it does not seem to work.  I have tried to install it using pip, but it does not seem to work.  I have tried to install it using easy_install, but it does not seem to work.  I have tried to install it using\n",
      "\n",
      "Eiffel Tower is in Paris.\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Eiffel Tower is in Paris.  The Eiffel Tower is the tallest building in the world. It is located in Paris, France. It is the tallest building in the world by a wide margin. It is the tallest building in the world by a wide margin. It is the tallest building in the world by a wide margin. It is the tallest building in the world by a wide margin. It is the tallest building in the world by a wide margin. It is the tallest building in the world by a wide margin. It is the tallest building in the world by a wide margin. It is the tallest building in the world by a wide margin. It is the tallest building in the world by a wide margin. It is the tallest building in the world by a wide margin. It is the tallest building in the world by a wide margin. It is the tallest building in the world by a wide margin. It is the\n",
      "\n",
      "The newly elected senator of Arizona\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>The newly elected senator of Arizona's 2nd Congressional District, Rep. Kyrsten Sinema, is a Democrat who has been a vocal critic of President Donald Trump.  She's also a former prosecutor who has been a vocal critic of the Trump administration.  Sinema, who is running for the seat being vacated by Republican Jeff Flake, has been a vocal critic of the Trump administration.  She's also a former prosecutor who has been a vocal critic of the Trump administration.  Sinema, who is running for the seat being vacated by Republican Jeff Flake, has been a vocal critic of the Trump administration.  Sinema, who is running for the seat being vacated by Republican Jeff Flake, has been a vocal critic of the Trump administration.  Sinema, who is running for the seat being vacated by Republican Jeff Flake, has been a vocal critic of the Trump administration.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = [\n",
    "    \"Apple has recently released their iPhone 14 line of\",\n",
    "    \"Google has released Pixel 7\",\n",
    "    \"I am taking a Machine Learning class\",\n",
    "    \"Eiffel Tower is in Paris.\",\n",
    "    # \"The newly elected senator of Arizona\"\n",
    "]\n",
    "\n",
    "txt, ret_dict = model_utils.generate_fast(\n",
    "    model, tokenizer,\n",
    "    prompt,\n",
    "    argmax_greedy = True,\n",
    "    max_out_len= 200,\n",
    "    # debug=True,\n",
    "    # get_answer_tokens=True,\n",
    "\n",
    "    prompt_tuning = soft_embeddings\n",
    ")\n",
    "\n",
    "model_utils.print_formatted_results(prompt, txt, ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdeca5f8d90>]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3835239043501baad7b502b0573c70a3454f6c2753902e68361683a11a30d10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
