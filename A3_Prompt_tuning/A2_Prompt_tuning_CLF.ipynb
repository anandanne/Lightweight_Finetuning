{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import nethook\n",
    "from utils import model_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2-medium\"  # gpt2-{medium,large,xl} or EleutherAI/gpt-j-6B\n",
    "mt = model_utils.ModelAndTokenizer(MODEL_NAME, low_cpu_mem_usage=False)\n",
    "\n",
    "model = mt.model\n",
    "tokenizer = mt.tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple has recently released their iPhone 14 line of\n",
      "Apple has recently released their iPhone 14 line of smartphones, and they have been praised for being very user friendly, easy-to-use, and very easy to use. The new iPhone 14 comes equipped with an all-new design with the iPhone logo\n",
      "p(answer):  p(' devices'[4410])=0.2049, p(' phones'[9512])=0.1882, p(' smartphones'[18151])=0.1707, p(' hands'[2832])=0.0887, p(' products'[3186])=0.0367\n",
      "\n",
      "Goole has released Pixel 7\n",
      "Goole has released Pixel 7 and Pixel 8 devices in the past, but this latest version has a much more premium look.  The Pixel 7 is the first phone to be released with Google's new dual camera setup, which allows you to\n",
      "p(answer):  p(' and'[290])=0.1817, p(','[11])=0.0804, p('.'[13])=0.0659, p(' Plus'[8227])=0.0247, p(' on'[319])=0.0194\n",
      "\n",
      "I am taking a Machine Learning class\n",
      "I am taking a Machine Learning class in my university and I was wondering if you could help me out. I have no idea what I would like to do but I'm curious if you could give me some tips to help me. Thanks!  \n",
      "p(answer):  p(' at'[379])=0.198, p(' in'[287])=0.0797, p(' with'[351])=0.071, p('.'[13])=0.0633, p(' and'[290])=0.0631\n",
      "\n",
      "Eiffel Tower is in Paris.\n",
      "Eiffel Tower is in Paris.  A French woman was killed and a police officer was injured after the gunman opened fire at the Eiffel Tower in Paris.  A man was killed and a police officer was injured after a gunman\n",
      "p(answer):  p('\n",
      "'[198])=0.2083, p(' The'[383])=0.1101, p(' It'[632])=0.075, p(' In'[554])=0.0222, p(' I'[314])=0.0214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = [\n",
    "    \"Apple has recently released their iPhone 14 line of\",\n",
    "    \"Goole has released Pixel 7\",\n",
    "    \"I am taking a Machine Learning class\",\n",
    "    \"Eiffel Tower is in Paris.\"\n",
    "]\n",
    "\n",
    "txt, ret_dict = model_utils.generate_fast(\n",
    "    model, tokenizer,\n",
    "    prompt,\n",
    "    argmax_greedy = False,\n",
    "    max_out_len= 50,\n",
    "    # debug=True,\n",
    "    get_answer_tokens=True,\n",
    ")\n",
    "\n",
    "model_utils.print_formatted_results(prompt, txt, ret_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader (the `GoEmotions` dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47752</th>\n",
       "      <td>Someone, some day, should do a study of archit...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23721</th>\n",
       "      <td>I bought a DVD collection (9 movies for 10 Eur...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5050</th>\n",
       "      <td>I totally disagree with the other reviews.All ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17340</th>\n",
       "      <td>Blade was a thrilling horror masterpiece and i...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45765</th>\n",
       "      <td>People seem to be expecting Citizen Kane here!...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "47752  Someone, some day, should do a study of archit...  negative\n",
       "23721  I bought a DVD collection (9 movies for 10 Eur...  negative\n",
       "5050   I totally disagree with the other reviews.All ...  positive\n",
       "17340  Blade was a thrilling horror masterpiece and i...  positive\n",
       "45765  People seem to be expecting Citizen Kane here!...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Data/IMDB_50K_Reviews/archive/IMDB Dataset.csv\")\n",
    "df = df.sample(frac = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Someone, some day, should do a study of architecture as it figures in horror films; of all those explorations of weirdly laid out mansions, searches for secret passageways and crypts, trackings of monsters through air ducts, and so forth. Offhand I can recall only a few films in which architecture played a major role throughout--\"Demon Seed,\" \"Cube,\" the remake of \"Thirteen Ghosts\"--but it's at the heart of every story about a spooky house or church or crypt; it's all about the character and the affect of spaces, passages, and walls. So I was looking forward to this thriller where it promised to be central. The idea is this: An architect has built--actually, rebuilt--for himself a huge and rambling house; his wife has just left him, mainly because of his own self-centeredness, but also, it is intimated, because she can't get used to the place since he remodeled it. Living in unaccustomed solitude (real this time, rather than virtual), he comes to suspect that somebody else--a stranger who had come to the door one evening asking to use the phone and then suddenly disappeared--is living into the house with him; only the place is big enough so that he never sees him.<br /><br />This is a good start for a melodrama, whose development one would expect to follow some such lines as these: After searching the house for the intruder a few times without success, the architect resorts to his blueprints to undertake more systematic searches, trying in various ways to surprise, intercept, or ambush the intruder, maybe by means of some special features he built into the structure. Meanwhile the intruder has discovered hiding places and back ways between places that the architect didn't foresee or doesn't remember. The movie would turn into a cat-and-mouse game, a hunt, a battle; and finally, in trying to trap the intruder, the architect himself would end up trapped in his own creation, in some way he didn't expect. Then he would be forced to think himself out of it--and maybe at the same time out of his own self-imposed isolation--and in a final twist would nail, and maybe even kill, the ****er.<br /><br />Nothing like this happens in this movie; the house is just a house, the architect is just a guy, and his nemesis is of an unknown character, if he exists at all. Here is what does happen in the movie: Once the intruder is installed in the house--if he is--the architect begins hearing noises, but when he goes to investigate finds nothing. He calls the police, they think he's slightly nuts; he persuades his estranged wife to spend the night, she thinks he's more nuts. At last, more or less accidentally, he runs into the intruder (doesn't get a good look, but figures, who else could it be?--not a hard question, in a story with, to that point, fewer than three principal characters), whereupon he locks the doors, lowers the grills on the windows, throws away the key (I don't know why he thought this necessary), and leaves his victim to starve. I missed why this was a given: the doors and walls are made of steel? In any event, the architect takes to sleeping in his car. And since the idea of the movie has languished undeveloped and cannot now be developed further, something else must be devised to take its place. And this is it: The architect--are you ready?--moves into the house of the man who (presumably) moved into his, and lives there in the same way. How is this possible? It is not, but the movie takes this route to try and make it seem so: The architect has drawn a picture of the man who came to his door; and when he leaves the house he takes the picture with him; and while sitting in his car, he throws the picture into the street; and two kids pick it up and observe that it looks like Martin, their neighbor; whereupon the architect asks where his house is and the kids point the way.<br /><br />If this sequence seems to verge on the implausible, what ensues plunges right in. The architect takes up residence with Martin's wheelchair-ridden wife, unbeknownst to her; so stealthy in his moves and so cunning in his reading of his hostess that he's able always to leave a room just as she enters or to duck out of sight just as she turns around. Throughout this section the movie is clever in one way, making (or leaving it to the viewer to make) the point that his life with this stranger, who doesn't know he's there, is in essence the same life he lived with his wife, as a virtual recluse with her as a convenient buffer. But at the same time, his inability to live in the world makes his transformation into Raffles the cat-burglar entirely incredible. Not to go into the series of twists at the end--including another murder achieved by locking someone in behind another invincible door--this one in front of a landing so flimsy that it collapses under the weight of a wheelchair; two nice people who take murder in stride; and (before the story started) the unnoticed construction of a tunnel under several houses.... To the final, long-anticipated twist, the movie adds another, to make it even more offensive, and then...ends.<br /><br />Here is a story that depends on the development of two things--the idea of the stranger in the house, and the character of the man whose house it is--and fumbles both. The first fumble makes it boring; the second made me angry, as it pushed its main character farther and farther along a more and more zigzaggy path, and never offered any explanation for the character who most required one: Martin the tunnel-builder and sneak-tenant. The story should be redone by someone, some day.\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    print(row[\"review\"])\n",
    "    print(row[\"sentiment\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[0:30000]\n",
    "validation_df = df[30000:40000]\n",
    "test_df = df[40000:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30000, 2), (10000, 2), (10000, 2))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, validation_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import re\n",
    "CLEANR = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});|/.*/')\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "  raw_html = raw_html.replace(\"\\\\\", \"\")\n",
    "  raw_html = raw_html.replace(\"&#039;\", \"\\'\")\n",
    "  cleantext = re.sub(CLEANR, ' ', raw_html)\n",
    "  split = cleantext.strip().split(\" \")\n",
    "  if(split[0].isnumeric()):\n",
    "    split = split[1:]\n",
    "  return \" \".join([w for w in split if len(w.strip()) > 0])\n",
    "\n",
    "# cleanhtml(\"Don&#039;t mess with me\")\n",
    "# cleanhtml('<a href=\"#p79290593\" class=\"quotelink\">&gt;&gt;79290593</a><br><span class=\"quote\">&gt;canada</span><br><br>and you faggots think we&#039;re the worst shit posters')\n",
    "\n",
    "class GoEmotions(Dataset):\n",
    "    def __init__(self, data_frame):\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "\n",
    "        for index, row in data_frame.iterrows():\n",
    "            self.x.append(\"<REVIEW>: \" + cleanhtml(row[\"review\"]) + \" <SENTIMENT>\")\n",
    "            self.y.append(\" \" + row[\"sentiment\"])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 10000, 10000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset = GoEmotions(train_df)\n",
    "validation_dataset = GoEmotions(validation_df)\n",
    "test_dataset = GoEmotions(test_df)\n",
    "\n",
    "len(training_dataset), len(validation_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=batch_size)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size)\n",
    "testing_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = \"transformer.wte\"\n",
    "layer_norm_final = \"transformer.ln_f\"\n",
    "unembedder = \"lm_head\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################\n",
    "prefix_size = 20\n",
    "num_epochs = 10\n",
    "###############################################################################################################\n",
    "\n",
    "learning_rate = 5e-4\n",
    "warmup_steps = 200\n",
    "weight_decay = 0\n",
    "\n",
    "optimization_batch_size = 8\n",
    "max_token_per_comment = 963\n",
    "\n",
    "save_path = f\"../Saved_weights/Promt-Tuned_CLF__IMDB_50K/{MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder_module = nethook.get_module(model, embedder)\n",
    "lm_head = nethook.get_module(model, unembedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 1024])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "init_words = [\"sentiment\", \"elephant\", \"review\"]\n",
    "def get_initial_prefix(prefix_size = 5):\n",
    "    words = random.choices(init_words, k=prefix_size)\n",
    "    sentence = \" \" + \" \".join(words)\n",
    "    tokenized = tokenizer(sentence, return_tensors = \"pt\").to(next(model.parameters()).device)\n",
    "    return embedder_module(tokenized['input_ids'])\n",
    "\n",
    "get_initial_prefix(7).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.n_embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "torch.Size([2, 23, 1024]) torch.Size([2, 23, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "soft_embeddings = get_initial_prefix(prefix_size)\n",
    "init_state = copy.deepcopy(soft_embeddings)\n",
    "soft_embeddings.requires_grad  = True\n",
    "\n",
    "print((init_state - soft_embeddings).norm())\n",
    "\n",
    "# def insert_prompt_embeddings(output, layer, soft_embeddings = soft_embeddings):\n",
    "#     if(layer != embedder):\n",
    "#         return output\n",
    "#     print(output.requires_grad, soft_embeddings.requires_grad)\n",
    "#     prefix_size = soft_embeddings.shape[1]\n",
    "#     for batch in output:\n",
    "#         batch[0:prefix_size] = soft_embeddings\n",
    "#     return output\n",
    "\n",
    "def insert_prompt_embeddings_2(output, layer, soft_embeddings = soft_embeddings):\n",
    "    if(layer != embedder):\n",
    "        return output\n",
    "    prefix_size = soft_embeddings.shape[1]\n",
    "    arr = []\n",
    "    for batch in output:\n",
    "        added = torch.cat((soft_embeddings[0], batch[prefix_size:, :]))\n",
    "        arr.append(added)\n",
    "    return torch.stack(arr)\n",
    "\n",
    "inner_rep = torch.randn([2, 23, model.config.n_embd]).to(next(model.parameters()).device)\n",
    "prefix_added = insert_prompt_embeddings_2(inner_rep, embedder)\n",
    "\n",
    "print(inner_rep.shape, prefix_added.shape)\n",
    "(inner_rep[..., prefix_size:, :] - prefix_added[..., prefix_size:, :]).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2413, -0.0916, -0.0155,  ...,  0.0148, -0.0138, -0.1585],\n",
      "         [-0.1227, -0.0664,  0.1448,  ...,  0.0807,  0.0043, -0.0249],\n",
      "         [-0.1227, -0.0664,  0.1448,  ...,  0.0807,  0.0043, -0.0249],\n",
      "         ...,\n",
      "         [-0.0402, -0.0731,  0.0385,  ...,  0.0422, -0.0059, -0.0381],\n",
      "         [-0.0402, -0.0731,  0.0385,  ...,  0.0422, -0.0059, -0.0381],\n",
      "         [-0.2413, -0.0916, -0.0155,  ...,  0.0148, -0.0138, -0.1585]]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(soft_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15000 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1233 > 1024). Running this sequence through the model will result in indexing errors\n",
      "  6%|▌         | 866/15000 [01:47<29:16,  8.05it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A2_Prompt_tuning_CLF.ipynb Cell 22\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A2_Prompt_tuning_CLF.ipynb#X23sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A2_Prompt_tuning_CLF.ipynb#X23sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mfor\u001b[39;00m reviews, sentiments \u001b[39min\u001b[39;00m tqdm(training_dataloader):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A2_Prompt_tuning_CLF.ipynb#X23sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     tokenized_inputs \u001b[39m=\u001b[39m tokenizer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A2_Prompt_tuning_CLF.ipynb#X23sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m         \u001b[39mlist\u001b[39;49m(reviews),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A2_Prompt_tuning_CLF.ipynb#X23sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         padding \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A2_Prompt_tuning_CLF.ipynb#X23sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A2_Prompt_tuning_CLF.ipynb#X23sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     )\u001b[39m.\u001b[39;49mto(\u001b[39mnext\u001b[39;49m(model\u001b[39m.\u001b[39;49mparameters())\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A2_Prompt_tuning_CLF.ipynb#X23sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39m# add soft tokens\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A2_Prompt_tuning_CLF.ipynb#X23sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     prefix_tokens \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(\u001b[39mlen\u001b[39m(reviews), prefix_size, dtype \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39mnext\u001b[39m(model\u001b[39m.\u001b[39mparameters())\u001b[39m.\u001b[39mdevice) \u001b[39m*\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mbos_token_id\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/transformers/file_utils.py:2134\u001b[0m, in \u001b[0;36mtorch_required.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2131\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m   2132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   2133\u001b[0m     \u001b[39mif\u001b[39;00m is_torch_available():\n\u001b[0;32m-> 2134\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2136\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMethod `\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m` requires PyTorch.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:744\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[39m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \u001b[39m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    742\u001b[0m \u001b[39m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m _is_torch_device(device) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mint\u001b[39m):\n\u001b[0;32m--> 744\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m    745\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    746\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(device)\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:744\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[39m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \u001b[39m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    742\u001b[0m \u001b[39m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m _is_torch_device(device) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mint\u001b[39m):\n\u001b[0;32m--> 744\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39;49mto(device\u001b[39m=\u001b[39;49mdevice) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m    745\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    746\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(device)\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "# optimizer = AdamW(\n",
    "#     # model.parameters(),\n",
    "#     [v for _, v in tunable_weights.items()],\n",
    "#     lr = learning_rate,\n",
    "# )\n",
    "# scheduler = get_linear_schedule_with_warmup(\n",
    "#     optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
    "# )\n",
    "\n",
    "\n",
    "for name, w in model.named_parameters():\n",
    "    w.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    [soft_embeddings],\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "\n",
    "num_prompts_optimized = 0\n",
    "training_loss_track = []\n",
    "validation_loss_track = []\n",
    "\n",
    "target_track = {\n",
    "    \" positive\": 0,\n",
    "    \" negative\": 0\n",
    "}\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    for reviews, sentiments in tqdm(training_dataloader):\n",
    "        tokenized_inputs = tokenizer(\n",
    "            list(reviews),\n",
    "            padding = True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(next(model.parameters()).device)\n",
    "\n",
    "        # add soft tokens\n",
    "        prefix_tokens = torch.ones(len(reviews), prefix_size, dtype = int).to(next(model.parameters()).device) * model.config.bos_token_id\n",
    "        tokenized_inputs[\"input_ids\"] = torch.cat((prefix_tokens, tokenized_inputs[\"input_ids\"]), dim = 1)\n",
    "        prefix_attn = torch.ones(len(reviews), prefix_size, dtype = int).to(next(model.parameters()).device)\n",
    "        tokenized_inputs[\"attention_mask\"] = torch.cat((prefix_attn, tokenized_inputs[\"attention_mask\"]), dim = 1)\n",
    "\n",
    "        if(tokenized_inputs['input_ids'].shape[1] > max_token_per_comment):\n",
    "            # print(f\"BLOCKED ==> {tokenized_inputs['input_ids'].shape[1]}\")\n",
    "            continue\n",
    "            \n",
    "        for t in sentiments:\n",
    "            target_track[t] += 1\n",
    "\n",
    "        target_ids = tokenizer(\n",
    "            list(sentiments), \n",
    "            padding = True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(next(model.parameters()).device)['input_ids']\n",
    "\n",
    "        # print(sentiments)\n",
    "\n",
    "        # print(tokenized_inputs['input_ids'].shape)\n",
    "        # print(sentiments, target_ids)\n",
    "\n",
    "        last_token_inds = tokenized_inputs[\"attention_mask\"].sum(dim=1) - 1\n",
    "        loss_mask = target_ids != tokenizer.unk_token_id\n",
    "\n",
    "        # tokenized[\"input_ids\"].require_grad = True\n",
    "        with nethook.TraceDict(\n",
    "            model,\n",
    "            [embedder, layer_norm_final, unembedder],\n",
    "            edit_output=insert_prompt_embeddings_2\n",
    "        ) as traces:\n",
    "            outputs = model(\n",
    "                **tokenized_inputs, \n",
    "                labels=tokenized_inputs['input_ids']\n",
    "            )\n",
    "\n",
    "        probs = torch.nn.functional.log_softmax(\n",
    "            outputs.logits[torch.arange(batch_size), last_token_inds], dim=-1\n",
    "        )\n",
    "        # print(probs)\n",
    "\n",
    "        loss = -(torch.gather(probs, 1, target_ids) * loss_mask).sum(1) / loss_mask.sum(1)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        training_loss_track.append(loss.item())\n",
    "\n",
    "        # print(loss)\n",
    "        # break\n",
    "\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # break\n",
    "        \n",
    "    print(\"#####################  CHECKPOINT -- saving weights #####################\")\n",
    "    os.makedirs(save_path, exist_ok = True)\n",
    "    torch.save(soft_embeddings, f\"{save_path}/promptuned__epoch_{epoch+1}.pth\")\n",
    "    with open(f\"{save_path}/loss_track_{num_prompts_optimized}.json\", \"w\") as f:\n",
    "        json.dump({\"training\": training_loss_track, \"validation\": validation_loss_track}, f)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.9214, device='cuda:0', grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(soft_embeddings - init_state).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdba6ba2370>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(training_loss_track)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('../Saved_weights/Fine-Tuned_CLF__IMDB_50K/gpt2-medium/finetuned_0.pth')\n",
    "# model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<REVIEW>: This was an awesome movie <SENTIMENT>\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><REVIEW>: This was an awesome movie <SENTIMENT> positive positive positive positive positive\n",
      "p(answer):  p(' positive'[3967])=0.9138, p(' negative'[4633])=0.0748, p('\n",
      "'[198])=0.0013, p(' I'[314])=0.0013, p(' good'[922])=0.0013\n",
      "\n",
      "<REVIEW>: This was a bad movie <SENTIMENT>\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><REVIEW>: This was a bad movie <SENTIMENT> negative negative negative negative negative\n",
      "p(answer):  p(' negative'[4633])=0.5217, p(' positive'[3967])=0.4695, p(' good'[922])=0.0015, p('\n",
      "'[198])=0.001, p(' I'[314])=0.0007\n",
      "\n",
      "<REVIEW>: This was not a good movie <SENTIMENT>\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><REVIEW>: This was not a good movie <SENTIMENT> positive positive positive positive\n",
      "p(answer):  p(' positive'[3967])=0.5973, p(' negative'[4633])=0.3931, p(' good'[922])=0.0015, p(' I'[314])=0.001, p('\n",
      "'[198])=0.0009\n",
      "\n",
      "<REVIEW>: That movie was garbage <SENTIMENT>\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><REVIEW>: That movie was garbage <SENTIMENT> negative negative negative negative negative negative\n",
      "p(answer):  p(' negative'[4633])=0.5938, p(' positive'[3967])=0.3946, p('\n",
      "'[198])=0.0018, p(' I'[314])=0.0012, p(' good'[922])=0.001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = [\n",
    "    \"This was an awesome movie\",\n",
    "    \"This was a bad movie\",\n",
    "    \"This was not a good movie\",\n",
    "    \"That movie was garbage\"\n",
    "]\n",
    "\n",
    "prompt = [\"<REVIEW>: \" + p + \" <SENTIMENT>\" for p in prompt]\n",
    "\n",
    "txt, ret_dict = model_utils.generate_fast(\n",
    "    model, tokenizer,\n",
    "    prompt,\n",
    "    argmax_greedy = True,\n",
    "    max_out_len= 40,\n",
    "    # debug=True,\n",
    "    get_answer_tokens=True,\n",
    "\n",
    "    prompt_tuning = soft_embeddings,\n",
    "    # track_interesting_words = [\n",
    "    #     [\" positive\", \" negative\"],\n",
    "    #     [\" positive\", \" negative\"]\n",
    "    # ]\n",
    ")\n",
    "\n",
    "model_utils.print_formatted_results(prompt, txt, ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12/2000 [00:44<2:03:44,  3.73s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A2_Prompt_tuning_CLF.ipynb Cell 28\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A2_Prompt_tuning_CLF.ipynb#X32sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m max_out_len \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(last_token_inds)\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A2_Prompt_tuning_CLF.ipynb#X32sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A2_Prompt_tuning_CLF.ipynb#X32sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     txt, ret_dict \u001b[39m=\u001b[39m model_utils\u001b[39m.\u001b[39;49mgenerate_fast(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A2_Prompt_tuning_CLF.ipynb#X32sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         model, tokenizer,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A2_Prompt_tuning_CLF.ipynb#X32sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         \u001b[39mlist\u001b[39;49m(reviews),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A2_Prompt_tuning_CLF.ipynb#X32sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         argmax_greedy \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A2_Prompt_tuning_CLF.ipynb#X32sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         max_out_len\u001b[39m=\u001b[39;49m max_out_len \u001b[39m+\u001b[39;49m prefix_size \u001b[39m+\u001b[39;49m \u001b[39m3\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A2_Prompt_tuning_CLF.ipynb#X32sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         \u001b[39m# debug=True,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A2_Prompt_tuning_CLF.ipynb#X32sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         get_answer_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A2_Prompt_tuning_CLF.ipynb#X32sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A2_Prompt_tuning_CLF.ipynb#X32sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         prompt_tuning \u001b[39m=\u001b[39;49m soft_embeddings,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A2_Prompt_tuning_CLF.ipynb#X32sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A2_Prompt_tuning_CLF.ipynb#X32sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mfor\u001b[39;00m t, p \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mlist\u001b[39m(sentiment), ret_dict[\u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A3_Prompt_tuning/A2_Prompt_tuning_CLF.ipynb#X32sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     target\u001b[39m.\u001b[39mappend(t)\n",
      "File \u001b[0;32m~/Codes/EmoGPT/A3_Prompt_tuning/../utils/model_utils.py:143\u001b[0m, in \u001b[0;36mgenerate_fast\u001b[0;34m(model, tok, prompts, top_k, max_out_len, argmax_greedy, debug, get_answer_tokens, track_interesting_words, prompt_tuning, embedder)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mwhile\u001b[39;00m input_ids\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m) \u001b[39m<\u001b[39m max_out_len:  \u001b[39m# while not exceeding max output length\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[39mwith\u001b[39;00m nethook\u001b[39m.\u001b[39mTraceDict(\n\u001b[1;32m    141\u001b[0m         model, [embedder], edit_output\u001b[39m=\u001b[39m intervention_function\n\u001b[1;32m    142\u001b[0m     ) \u001b[39mas\u001b[39;00m traces:\n\u001b[0;32m--> 143\u001b[0m         model_out \u001b[39m=\u001b[39m model(\n\u001b[1;32m    144\u001b[0m             input_ids\u001b[39m=\u001b[39;49minput_ids[:, cur_context],\n\u001b[1;32m    145\u001b[0m             attention_mask\u001b[39m=\u001b[39;49mattention_mask[:, cur_context],\n\u001b[1;32m    146\u001b[0m             past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    147\u001b[0m             use_cache\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    148\u001b[0m         )\n\u001b[1;32m    149\u001b[0m     \u001b[39mif\u001b[39;00m(intervention_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    150\u001b[0m         intervention_function \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1044\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[39m    `-100` are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1044\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1045\u001b[0m     input_ids,\n\u001b[1;32m   1046\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1047\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1048\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1049\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1050\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1051\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1052\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1053\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1054\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1055\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1056\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1057\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1058\u001b[0m )\n\u001b[1;32m   1059\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1061\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:887\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    877\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    878\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    879\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    884\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    885\u001b[0m     )\n\u001b[1;32m    886\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 887\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    888\u001b[0m         hidden_states,\n\u001b[1;32m    889\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    890\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    891\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    892\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    893\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    894\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    895\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    896\u001b[0m     )\n\u001b[1;32m    898\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    899\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:432\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    430\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    431\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 432\u001b[0m feed_forward_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states)\n\u001b[1;32m    433\u001b[0m \u001b[39m# residual connection\u001b[39;00m\n\u001b[1;32m    434\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:360\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[1;32m    359\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_fc(hidden_states)\n\u001b[0;32m--> 360\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(hidden_states)\n\u001b[1;32m    361\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(hidden_states)\n\u001b[1;32m    362\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/transformers/activations.py:42\u001b[0m, in \u001b[0;36mgelu_new\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgelu_new\u001b[39m(x):\n\u001b[1;32m     38\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0.5\u001b[39;49m \u001b[39m*\u001b[39;49m x \u001b[39m*\u001b[39;49m (\u001b[39m1.0\u001b[39;49m \u001b[39m+\u001b[39;49m torch\u001b[39m.\u001b[39;49mtanh(math\u001b[39m.\u001b[39;49msqrt(\u001b[39m2.0\u001b[39;49m \u001b[39m/\u001b[39;49m math\u001b[39m.\u001b[39;49mpi) \u001b[39m*\u001b[39;49m (x \u001b[39m+\u001b[39;49m \u001b[39m0.044715\u001b[39;49m \u001b[39m*\u001b[39;49m torch\u001b[39m.\u001b[39;49mpow(x, \u001b[39m3.0\u001b[39;49m))))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "testing_dataloader = DataLoader(test_dataset, batch_size=5)\n",
    "\n",
    "target = []\n",
    "predict = []\n",
    "\n",
    "for reviews, sentiment in tqdm(testing_dataloader):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        list(reviews),\n",
    "        padding = True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(next(model.parameters()).device)\n",
    "\n",
    "    if(tokenized_inputs['input_ids'].shape[1] > max_token_per_comment):\n",
    "        # print(f\"BLOCKED ==> {tokenized_inputs['input_ids'].shape[1]}\")\n",
    "        continue\n",
    "\n",
    "    last_token_inds = tokenized_inputs[\"attention_mask\"].sum(dim=1)\n",
    "    max_out_len = max(last_token_inds).item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        txt, ret_dict = model_utils.generate_fast(\n",
    "            model, tokenizer,\n",
    "            list(reviews),\n",
    "            argmax_greedy = True,\n",
    "            max_out_len= max_out_len + prefix_size + 3,\n",
    "            # debug=True,\n",
    "            get_answer_tokens=True,\n",
    "\n",
    "            prompt_tuning = soft_embeddings,\n",
    "        )\n",
    "\n",
    "    for t, p in zip(list(sentiment), ret_dict['answer']):\n",
    "        target.append(t)\n",
    "        predict.append(p['top_token'])\n",
    "\n",
    "    # print(txt, ret_dict['answer'])\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 1\n",
      "2 32\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(target, predict).ravel()\n",
    "\n",
    "print(tp, fp)\n",
    "print(fn, tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9393939393939394"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity = tp/(tp + fn)\n",
    "specificity = tn/(tn + fp)\n",
    "balanced_acc = (sensitivity + specificity)/2\n",
    "\n",
    "balanced_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3835239043501baad7b502b0573c70a3454f6c2753902e68361683a11a30d10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
