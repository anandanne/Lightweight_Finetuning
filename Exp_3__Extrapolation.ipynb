{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "import re\n",
    "import os\n",
    "\n",
    "from utils import testing_utils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils import model_utils\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"Saved_weights/EXP3/\"\n",
    "os.makedirs(save_path, exist_ok= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gpt2-medium initialized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"gpt2-medium\"\n",
    "mt = model_utils.ModelAndTokenizer(MODEL_NAME, low_cpu_mem_usage=False)\n",
    "model = mt.model\n",
    "tokenizer = mt.tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f\"Model {MODEL_NAME} initialized\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "test_df = getDF('Data/Amazon_reviews/reviews_Electronics_5.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[(test_df['overall']==1.0) | (test_df['overall']==5.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "CLEANR = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});|/.*/')\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    raw_html = raw_html.replace(\"\\\\\", \"\")\n",
    "    raw_html = raw_html.replace(\"&#039;\", \"\\'\")\n",
    "    cleantext = re.sub(CLEANR, ' ', raw_html)\n",
    "    split = cleantext.strip().split(\" \")\n",
    "    if(split[0].isnumeric()):\n",
    "      split = split[1:]\n",
    "    return \" \".join([w for w in split if len(w.strip()) > 0])\n",
    "\n",
    "class Amazon_Review(Dataset):\n",
    "    def __init__(self, data_frame, limiter = 20000):\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        self.limiter = limiter\n",
    "        self.counter = {\n",
    "            \"positive\": 0,\n",
    "            \"negative\": 0\n",
    "        }\n",
    "\n",
    "        for index, row in tqdm(data_frame.iterrows()):\n",
    "            sentiment = \"positive\" if row[\"overall\"] == 5.0 else \"negative\"\n",
    "            if(self.counter[sentiment] == self.limiter):\n",
    "                break_loop = True\n",
    "                for k in self.counter:\n",
    "                    if(self.counter[k] != self.limiter):\n",
    "                      break_loop = False\n",
    "                      break\n",
    "                if(break_loop):\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            self.x.append(\"<REVIEW>: \" + cleanhtml(row[\"reviewText\"]) + \" <SENTIMENT>\")\n",
    "            self.y.append(f\" {sentiment}\")\n",
    "            self.counter[sentiment] += 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "205923it [00:05, 38544.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = Amazon_Review(test_df.sample(frac = 1))\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"<REVIEW>: I have been using theTranscend 32 GB Class 10 SDHC Flash Memory Card (TS32GSDHC10E)for some time now, and have been quite pleased on a price to performance basis. However, as the photo count on an individual card goes up, so does the amount of time I spend staring blankly at my computer screen while the photos transfer. I was interested to see how much of an improvement could be realized by stepping up to the SanDisk Extreme.While I realize that I am comparing apples to oranges, the performance of the SanDisk Extreme really blew my hair back. I now understand the price differential. The read speed, which I expected to be substantially better, did not disappoint. Though, it was the write speed that really opened my eyes.An occasion arose to photograph a large family with eight children under the age of ten. I figured this would provide the perfect proving ground to test the Extreme's mettle. I realized almost instantly that I would not have been able to shoot at anywhere close to the rate I was able to achieve with the Extreme if I had used my tried and true Transcend.This will definitely sound nave to the pros and serious amateurs out there, but the SanDisk Extreme has helped me realize that one's choice of memory card matters in digital photography. While I will probably continue to use the economical Transcends for still life, I will certainly be stocking my bag with a few Extreme's for subject matter with a pulse. To be honest, I am now quite curious about what can be achieved by stepping up to theSanDisk Extreme Pro 32 GB SDHC Class 10 UHS-1 Flash Memory Card 95MB/s SDSDXPA-032G-AFFP! <SENTIMENT>\",\n",
       " ' positive')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataloader = DataLoader(test_dataset, batch_size = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapter-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "testing .... \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 573/40000 [00:26<29:25, 22.34it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2369 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 40000/40000 [30:37<00:00, 21.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 18540, 'fn': 1398, 'fp': 1677, 'tn': 18240}\n",
      "0.9228416042737457\n"
     ]
    }
   ],
   "source": [
    "from utils.tuning_utils import Adapter\n",
    "adapters = torch.load(\"Saved_weights/Final/Adapters/gpt2-medium/adapter_dim__32.pth\")\n",
    "\n",
    "test_results = testing_utils.test(\n",
    "    testing_dataloader,\n",
    "    model, tokenizer,\n",
    "    light_weight_tuning = adapters, algo = \"adapter\",\n",
    "    # limit = 1000\n",
    ")\n",
    "\n",
    "with open(save_path + \"adapter_extrapolated.json\", \"w\") as f:\n",
    "    json.dump(test_results, f)\n",
    "\n",
    "print(test_results[\"confusion_matrix\"])\n",
    "print(test_results[\"balanced_accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefix-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "testing .... \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [28:38<00:00, 23.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 19668, 'fn': 270, 'fp': 4163, 'tn': 15754}\n",
      "0.8887202987795076\n"
     ]
    }
   ],
   "source": [
    "prefix_embeddings = torch.load(\"Saved_weights/Final/Prefix_Tuning/gpt2-medium/prefix_size__2.pth\")\n",
    "\n",
    "test_results = testing_utils.test(\n",
    "    testing_dataloader,\n",
    "    model, tokenizer,\n",
    "    light_weight_tuning = prefix_embeddings, algo = \"prefix\",\n",
    "    prefix_size = 2\n",
    ")\n",
    "\n",
    "with open(save_path + \"prefix_extrapolated.json\", \"w\") as f:\n",
    "    json.dump(test_results, f)\n",
    "\n",
    "print(test_results[\"confusion_matrix\"])\n",
    "print(test_results[\"balanced_accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "testing .... \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [24:07<00:00, 27.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 19694, 'fn': 244, 'fp': 4203, 'tn': 15714}\n",
      "0.8883681527511608\n"
     ]
    }
   ],
   "source": [
    "soft_tokens = torch.load(\"Saved_weights/Final/Prompt_Tuning/gpt2-medium/prompt_size__8.pth\", map_location='cuda:0')\n",
    "\n",
    "test_results = testing_utils.test(\n",
    "    testing_dataloader,\n",
    "    model, tokenizer,\n",
    "    light_weight_tuning = soft_tokens, algo = \"prompt\",\n",
    "    prefix_size = soft_tokens.shape[1]\n",
    ")\n",
    "\n",
    "with open(save_path + \"promt_extrapolated.json\", \"w\") as f:\n",
    "    json.dump(test_results, f)\n",
    "\n",
    "print(test_results[\"confusion_matrix\"])\n",
    "print(test_results[\"balanced_accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Model gpt2-medium initialized\n"
     ]
    }
   ],
   "source": [
    "mt_2 = model_utils.ModelAndTokenizer(MODEL_NAME, low_cpu_mem_usage=False)\n",
    "model_2 = mt.model\n",
    "tokenizer_2 = mt.tokenizer\n",
    "tokenizer_2.pad_token = tokenizer_2.eos_token\n",
    "print(f\"Duplicate Model {MODEL_NAME} initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.load(\"Saved_weights/Final/Finetune/gpt2-medium/gpt2-medium____data_40000.pth\")\n",
    "model_2.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "testing .... \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [23:47<00:00, 28.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 17406, 'fn': 2532, 'fp': 8475, 'tn': 11442}\n",
      "0.7237452143216497\n"
     ]
    }
   ],
   "source": [
    "test_results = testing_utils.test(\n",
    "    testing_dataloader,\n",
    "    model_2, tokenizer_2,\n",
    ")\n",
    "\n",
    "with open(save_path + \"finetune_extrapolated.json\", \"w\") as f:\n",
    "    json.dump(test_results, f)\n",
    "\n",
    "print(test_results[\"confusion_matrix\"])\n",
    "print(test_results[\"balanced_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3835239043501baad7b502b0573c70a3454f6c2753902e68361683a11a30d10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
