{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import nethook\n",
    "from utils import model_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2-medium\"  # gpt2-{medium,large,xl} or EleutherAI/gpt-j-6B\n",
    "mt = model_utils.ModelAndTokenizer(MODEL_NAME, low_cpu_mem_usage=False)\n",
    "\n",
    "model = mt.model\n",
    "tokenizer = mt.tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple has recently released their iPhone 14 line of\n",
      "Apple has recently released their iPhone 14 line of phones, which are designed to run the latest iOS 9.0, but the company has yet to release a new iPhone model.  The latest version of Apple is called the iPhone 8, which is\n",
      "p(answer):  p(' devices'[4410])=0.2049, p(' phones'[9512])=0.1882, p(' smartphones'[18151])=0.1707, p(' hands'[2832])=0.0887, p(' products'[3186])=0.0367\n",
      "\n",
      "Goole has released Pixel 7\n",
      "Goole has released Pixel 7.1, which is a new update to Android Marshmallow that includes a lot of new features. The biggest new feature is a new \"Pixel Launcher\". It is a customisable launcher that is available from the settings\n",
      "p(answer):  p(' and'[290])=0.1817, p(','[11])=0.0804, p('.'[13])=0.0659, p(' Plus'[8227])=0.0247, p(' on'[319])=0.0194\n",
      "\n",
      "I am taking a Machine Learning class\n",
      "I am taking a Machine Learning class at Stanford University, and I have been doing it for years now. I am currently using Python as well as Caffe, but I have been using both of them for some time now. The reason why is because\n",
      "p(answer):  p(' at'[379])=0.198, p(' in'[287])=0.0797, p(' with'[351])=0.071, p('.'[13])=0.0633, p(' and'[290])=0.0631\n",
      "\n",
      "Eiffel Tower is in Paris.\n",
      "Eiffel Tower is in Paris. I'm sure you've heard of it. It was built by the French. The French were really good about it and it's still there. The French have been building it ever since.  It's\n",
      "p(answer):  p('\n",
      "'[198])=0.2083, p(' The'[383])=0.1101, p(' It'[632])=0.075, p(' In'[554])=0.0222, p(' I'[314])=0.0214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = [\n",
    "    \"Apple has recently released their iPhone 14 line of\",\n",
    "    \"Goole has released Pixel 7\",\n",
    "    \"I am taking a Machine Learning class\",\n",
    "    \"Eiffel Tower is in Paris.\"\n",
    "]\n",
    "\n",
    "txt, ret_dict = model_utils.generate_fast(\n",
    "    model, tokenizer,\n",
    "    prompt,\n",
    "    argmax_greedy = False,\n",
    "    max_out_len= 50,\n",
    "    # debug=True,\n",
    "    get_answer_tokens=True,\n",
    ")\n",
    "\n",
    "model_utils.print_formatted_results(prompt, txt, ret_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset (the `IMDB_50K_Reviews` dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38747</th>\n",
       "      <td>.... could it be that ITV wouldn't want to rel...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32984</th>\n",
       "      <td>A typical Lanza flick that had limited audienc...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10374</th>\n",
       "      <td>Kurt Thomas stars as Jonathan Cabot some kind ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32170</th>\n",
       "      <td>It's a gentle, easy-going 1950s comedy. Kim No...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21268</th>\n",
       "      <td>As you may have gathered from the title, I who...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "38747  .... could it be that ITV wouldn't want to rel...  positive\n",
       "32984  A typical Lanza flick that had limited audienc...  negative\n",
       "10374  Kurt Thomas stars as Jonathan Cabot some kind ...  negative\n",
       "32170  It's a gentle, easy-going 1950s comedy. Kim No...  positive\n",
       "21268  As you may have gathered from the title, I who...  negative"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Data/IMDB_50K_Reviews/archive/IMDB Dataset.csv\")\n",
    "df = df.sample(frac = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... could it be that ITV wouldn't want to release this absolute classic because it would show up their current series of Mike Bassett for what it is? When discussing Mike Bassett with some work colleagues I mentioned Bostock's Cup as being a far superior offering and was surprised to find that I seem to be the only person in my entire office that has actually seen it. This can't be right for a film that has got to be the funniest thing I have ever seen.<br /><br />Let's face it, ITV don't have the greatest recent record for producing comedy so you would think that they would jump at the chance of at least repeating something which is genuinely funny. Perhaps if it could be combined with a lucrative telephone competition of where we think the coach driver will go next then they might be interested.<br /><br />Come on ITV, there are still some of us out there who would like to watch original, quality comedy/drama. Do the decent thing thing. Repeat it then get it out on DVD.\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    print(row[\"review\"])\n",
    "    print(row[\"sentiment\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[0:30000]\n",
    "validation_df = df[30000:40000]\n",
    "test_df = df[40000:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30000, 2), (10000, 2), (10000, 2))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, validation_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load presaved Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../Data/IMDB_50K_Reviews/train.csv\")\n",
    "validation_df = pd.read_csv(\"../Data/IMDB_50K_Reviews/validation.csv\")\n",
    "test_df = pd.read_csv(\"../Data/IMDB_50K_Reviews/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import re\n",
    "CLEANR = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});|/.*/')\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "  raw_html = raw_html.replace(\"\\\\\", \"\")\n",
    "  raw_html = raw_html.replace(\"&#039;\", \"\\'\")\n",
    "  cleantext = re.sub(CLEANR, ' ', raw_html)\n",
    "  split = cleantext.strip().split(\" \")\n",
    "  if(split[0].isnumeric()):\n",
    "    split = split[1:]\n",
    "  return \" \".join([w for w in split if len(w.strip()) > 0])\n",
    "\n",
    "# cleanhtml(\"Don&#039;t mess with me\")\n",
    "# cleanhtml('<a href=\"#p79290593\" class=\"quotelink\">&gt;&gt;79290593</a><br><span class=\"quote\">&gt;canada</span><br><br>and you faggots think we&#039;re the worst shit posters')\n",
    "\n",
    "class GoEmotions(Dataset):\n",
    "    def __init__(self, data_frame):\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "\n",
    "        for index, row in data_frame.iterrows():\n",
    "            self.x.append(\"<REVIEW>: \" + cleanhtml(row[\"review\"]) + \" <SENTIMENT>\")\n",
    "            self.y.append(\" \" + row[\"sentiment\"])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 10000, 10000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset = GoEmotions(train_df)\n",
    "validation_dataset = GoEmotions(validation_df)\n",
    "test_dataset = GoEmotions(test_df)\n",
    "\n",
    "len(training_dataset), len(validation_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=batch_size)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size)\n",
    "testing_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = \"transformer.wte\"\n",
    "layer_norm_final = \"transformer.ln_f\"\n",
    "transformer_blocks = [f\"transformer.h.{n}\" for n in range(model.config.n_layer)]\n",
    "unembedder = \"lm_head\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################\n",
    "prefix_size = 10\n",
    "num_epochs = 10\n",
    "###############################################################################################################\n",
    "\n",
    "learning_rate = 5e-4\n",
    "warmup_steps = 200\n",
    "weight_decay = 0\n",
    "\n",
    "optimization_batch_size = 8\n",
    "max_token_per_comment = 963\n",
    "\n",
    "save_path = f\"../Saved_weights/Prefix-Tuned_CLF__IMDB_50K/{MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder_module = nethook.get_module(model, embedder)\n",
    "lm_head = nethook.get_module(model, unembedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 1024])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "init_words = [\"sentiment\", \"elephant\", \"review\"]\n",
    "def get_initial_prefix(prefix_size = 5):\n",
    "    words = random.choices(init_words, k=prefix_size)\n",
    "    sentence = \" \" + \" \".join(words)\n",
    "    tokenized = tokenizer(sentence, return_tensors = \"pt\").to(next(model.parameters()).device)\n",
    "    return embedder_module(tokenized['input_ids'])\n",
    "\n",
    "get_initial_prefix(7).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untuple(output):\n",
    "    if(type(output) is tuple):\n",
    "        return output[0]\n",
    "    return output\n",
    "\n",
    "def get_shape(output):\n",
    "    pre = f\"{type(output)} ==> \"\n",
    "    if(type(output) is tuple):\n",
    "        return pre + f\"{output[0].shape} -- {(output[1][0].shape, output[1][1].shape)}\"\n",
    "    return pre + f\"{output.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prefix_tuning_edit(prefix_embeddings):\n",
    "    def insert_prompt_embeddings(output, layer, prefix_embeddings = prefix_embeddings):\n",
    "        if(layer not in prefix_embeddings):\n",
    "            return output\n",
    "        # print(\"intervention ==> \", layer, \"output shape ===> \", get_shape(output))\n",
    "        # return output\n",
    "        X = untuple(output)\n",
    "        prefix_now = prefix_embeddings[layer]\n",
    "        prefix_size = prefix_now.shape[1]\n",
    "        arr = []\n",
    "        for batch in X:\n",
    "            added = torch.cat((prefix_now[0], batch[prefix_size:, :]))\n",
    "            arr.append(added)\n",
    "        X = torch.stack(arr)\n",
    "\n",
    "        if(type(output) is not tuple):\n",
    "            return X\n",
    "        else:\n",
    "            return (X, output[1])\n",
    "    return insert_prompt_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 1024])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "prefix_embeddings = {\n",
    "    key: get_initial_prefix(prefix_size) for key in [embedder] + transformer_blocks[:-1]\n",
    "}\n",
    "\n",
    "init_states = copy.deepcopy(prefix_embeddings)\n",
    "\n",
    "prefix_embeddings[embedder].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = [\n",
    "#     \"Apple has recently released their iPhone 14 line of\",\n",
    "#     \"Goole has released Pixel 7\",\n",
    "#     \"I am taking a Machine Learning class\",\n",
    "#     \"Eiffel Tower is in Paris.\"\n",
    "# ]\n",
    "\n",
    "# txt, ret_dict = model_utils.generate_fast(\n",
    "#     model, tokenizer,\n",
    "#     prompt,\n",
    "#     argmax_greedy = False,\n",
    "#     max_out_len= 50,\n",
    "#     get_answer_tokens=True,\n",
    "\n",
    "#     light_weight_tuning= prefix_embeddings, algo = \"prefix\"\n",
    "# )\n",
    "\n",
    "# model_utils.print_formatted_results(prompt, txt, ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/15000 [00:00<13:23, 18.67it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1036 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 12%|█▏        | 1855/15000 [02:57<17:07, 12.79it/s]"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# optimizer = AdamW(\n",
    "#     # model.parameters(),\n",
    "#     [v for _, v in tunable_weights.items()],\n",
    "#     lr = learning_rate,\n",
    "# )\n",
    "# scheduler = get_linear_schedule_with_warmup(\n",
    "#     optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
    "# )\n",
    "\n",
    "for k in prefix_embeddings:\n",
    "    prefix_embeddings[k].requires_grad = True\n",
    "\n",
    "for name, w in model.named_parameters():\n",
    "    w.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    [prefix_embeddings[k] for k in prefix_embeddings],\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "\n",
    "num_prompts_optimized = 0\n",
    "training_loss_track = []\n",
    "validation_loss_track = []\n",
    "\n",
    "target_track = {\n",
    "    \" positive\": 0,\n",
    "    \" negative\": 0\n",
    "}\n",
    "\n",
    "###############################################################################\n",
    "insert_prefix_embeddings = get_prefix_tuning_edit(prefix_embeddings)\n",
    "###############################################################################\n",
    "\n",
    "limit = 700\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    for reviews, sentiments in tqdm(training_dataloader):\n",
    "        tokenized_inputs = tokenizer(\n",
    "            list(reviews),\n",
    "            padding = True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(next(model.parameters()).device)\n",
    "\n",
    "        # add soft tokens\n",
    "        prefix_tokens = torch.ones(len(reviews), prefix_size, dtype = int).to(next(model.parameters()).device) * model.config.bos_token_id\n",
    "        tokenized_inputs[\"input_ids\"] = torch.cat((prefix_tokens, tokenized_inputs[\"input_ids\"]), dim = 1)\n",
    "        prefix_attn = torch.ones(len(reviews), prefix_size, dtype = int).to(next(model.parameters()).device)\n",
    "        tokenized_inputs[\"attention_mask\"] = torch.cat((prefix_attn, tokenized_inputs[\"attention_mask\"]), dim = 1)\n",
    "\n",
    "        if(tokenized_inputs['input_ids'].shape[1] > max_token_per_comment):\n",
    "            # print(f\"BLOCKED ==> {tokenized_inputs['input_ids'].shape[1]}\")\n",
    "            continue\n",
    "            \n",
    "        for t in sentiments:\n",
    "            target_track[t] += 1\n",
    "\n",
    "        target_ids = tokenizer(\n",
    "            list(sentiments), \n",
    "            padding = True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(next(model.parameters()).device)['input_ids']\n",
    "\n",
    "        # print(sentiments)\n",
    "\n",
    "        # print(tokenized_inputs['input_ids'].shape)\n",
    "        # print(sentiments, target_ids)\n",
    "\n",
    "        last_token_inds = tokenized_inputs[\"attention_mask\"].sum(dim=1) - 1\n",
    "        loss_mask = target_ids != tokenizer.unk_token_id\n",
    "\n",
    "        # tokenized[\"input_ids\"].require_grad = True\n",
    "        with nethook.TraceDict(\n",
    "            model,\n",
    "            [embedder, layer_norm_final] + transformer_blocks + [unembedder],\n",
    "            edit_output=insert_prefix_embeddings\n",
    "        ) as traces:\n",
    "            outputs = model(\n",
    "                **tokenized_inputs, \n",
    "                labels=tokenized_inputs['input_ids']\n",
    "            )\n",
    "\n",
    "        probs = torch.nn.functional.log_softmax(\n",
    "            outputs.logits[torch.arange(batch_size), last_token_inds], dim=-1\n",
    "        )\n",
    "        # print(probs)\n",
    "\n",
    "        loss = -(torch.gather(probs, 1, target_ids) * loss_mask).sum(1) / loss_mask.sum(1)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        training_loss_track.append(loss.item())\n",
    "        # print(loss)\n",
    "\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # limit -= 1\n",
    "        # if(limit == 0):\n",
    "        #     break\n",
    "        \n",
    "    print(\"#####################  CHECKPOINT -- saving weights #####################\")\n",
    "    os.makedirs(save_path, exist_ok = True)\n",
    "    torch.save(prefix_embeddings, f\"{save_path}/prefixtuned__epoch_{epoch+1}.pth\")\n",
    "    with open(f\"{save_path}/loss_track_{epoch + 1}.json\", \"w\") as f:\n",
    "        json.dump({\"training\": training_loss_track, \"validation\": validation_loss_track}, f)\n",
    "    \n",
    "    plt.title(\"\")\n",
    "    plt.plot(training_loss_track)\n",
    "    plt.show()\n",
    "    training_loss_track = []\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('../Saved_weights/Fine-Tuned_CLF__IMDB_50K/gpt2-medium/finetuned_0.pth')\n",
    "# model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<REVIEW>: This was an awesome movie <SENTIMENT>\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><REVIEW>: This was an awesome movie <SENTIMENT> negative negative negative negative negative negative negative negative negative negative negative negative negative negative negative\n",
      "p(answer):  p(' negative'[4633])=0.5699, p(' positive'[3967])=0.4261, p(' review'[2423])=0.0003, p(' Positive'[33733])=0.0003, p('positive'[24561])=0.0003\n",
      "\n",
      "<REVIEW>: This was a bad movie <SENTIMENT>\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><REVIEW>: This was a bad movie <SENTIMENT> negative negative negative negative negative negative negative negative negative negative negative negative negative negative negative\n",
      "p(answer):  p(' negative'[4633])=0.61, p(' positive'[3967])=0.386, p(' review'[2423])=0.0003, p(' Positive'[33733])=0.0003, p('positive'[24561])=0.0002\n",
      "\n",
      "<REVIEW>: This was not a good movie <SENTIMENT>\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><REVIEW>: This was not a good movie <SENTIMENT> negative negative negative negative negative negative negative negative negative negative negative negative negative negative\n",
      "p(answer):  p(' negative'[4633])=0.5653, p(' positive'[3967])=0.4305, p(' Positive'[33733])=0.0003, p(' review'[2423])=0.0003, p('positive'[24561])=0.0003\n",
      "\n",
      "<REVIEW>: That movie was garbage <SENTIMENT>\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><REVIEW>: That movie was garbage <SENTIMENT> negative negative negative negative negative negative negative negative negative negative negative negative negative negative negative negative\n",
      "p(answer):  p(' negative'[4633])=0.5996, p(' positive'[3967])=0.3966, p('positive'[24561])=0.0002, p(' Positive'[33733])=0.0002, p(' review'[2423])=0.0002\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = [\n",
    "    \"This was an awesome movie\",\n",
    "    \"This was a bad movie\",\n",
    "    \"This was not a good movie\",\n",
    "    \"That movie was garbage\"\n",
    "]\n",
    "\n",
    "prompt = [\"<REVIEW>: \" + p + \" <SENTIMENT>\" for p in prompt]\n",
    "\n",
    "txt, ret_dict = model_utils.generate_fast(\n",
    "    model, tokenizer,\n",
    "    prompt,\n",
    "    argmax_greedy = True,\n",
    "    max_out_len= 40,\n",
    "    # debug=True,\n",
    "    get_answer_tokens=True,\n",
    "\n",
    "    light_weight_tuning= prefix_embeddings, algo = \"prefix\"\n",
    "    # track_interesting_words = [\n",
    "    #     [\" positive\", \" negative\"],\n",
    "    #     [\" positive\", \" negative\"]\n",
    "    # ]\n",
    ")\n",
    "\n",
    "model_utils.print_formatted_results(prompt, txt, ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/2000 [00:51<3:33:27,  6.43s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/local_arnab/Codes/EmoGPT/A4_Prefix_tuning/Prefix_tuning_CLF.ipynb Cell 30\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A4_Prefix_tuning/Prefix_tuning_CLF.ipynb#X36sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m max_out_len \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(last_token_inds)\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A4_Prefix_tuning/Prefix_tuning_CLF.ipynb#X36sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A4_Prefix_tuning/Prefix_tuning_CLF.ipynb#X36sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     txt, ret_dict \u001b[39m=\u001b[39m model_utils\u001b[39m.\u001b[39;49mgenerate_fast(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A4_Prefix_tuning/Prefix_tuning_CLF.ipynb#X36sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         model, tokenizer,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A4_Prefix_tuning/Prefix_tuning_CLF.ipynb#X36sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         \u001b[39mlist\u001b[39;49m(reviews),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A4_Prefix_tuning/Prefix_tuning_CLF.ipynb#X36sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         argmax_greedy \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A4_Prefix_tuning/Prefix_tuning_CLF.ipynb#X36sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         max_out_len\u001b[39m=\u001b[39;49m max_out_len \u001b[39m+\u001b[39;49m prefix_size \u001b[39m+\u001b[39;49m \u001b[39m3\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A4_Prefix_tuning/Prefix_tuning_CLF.ipynb#X36sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         \u001b[39m# debug=True,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A4_Prefix_tuning/Prefix_tuning_CLF.ipynb#X36sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         get_answer_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A4_Prefix_tuning/Prefix_tuning_CLF.ipynb#X36sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A4_Prefix_tuning/Prefix_tuning_CLF.ipynb#X36sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         light_weight_tuning\u001b[39m=\u001b[39;49m prefix_embeddings, algo \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mprefix\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A4_Prefix_tuning/Prefix_tuning_CLF.ipynb#X36sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A4_Prefix_tuning/Prefix_tuning_CLF.ipynb#X36sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mfor\u001b[39;00m t, p \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mlist\u001b[39m(sentiment), ret_dict[\u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/local_arnab/Codes/EmoGPT/A4_Prefix_tuning/Prefix_tuning_CLF.ipynb#X36sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     target\u001b[39m.\u001b[39mappend(t)\n",
      "File \u001b[0;32m~/Codes/EmoGPT/A4_Prefix_tuning/../utils/model_utils.py:188\u001b[0m, in \u001b[0;36mgenerate_fast\u001b[0;34m(model, tok, prompts, top_k, max_out_len, argmax_greedy, debug, get_answer_tokens, track_interesting_words, light_weight_tuning, algo, embedder)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[39mwhile\u001b[39;00m input_ids\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m) \u001b[39m<\u001b[39m max_out_len:  \u001b[39m# while not exceeding max output length\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     \u001b[39mwith\u001b[39;00m nethook\u001b[39m.\u001b[39mTraceDict(\n\u001b[1;32m    186\u001b[0m         model, trace_modules, edit_output\u001b[39m=\u001b[39m intervention_function\n\u001b[1;32m    187\u001b[0m     ) \u001b[39mas\u001b[39;00m traces:\n\u001b[0;32m--> 188\u001b[0m         model_out \u001b[39m=\u001b[39m model(\n\u001b[1;32m    189\u001b[0m             input_ids\u001b[39m=\u001b[39;49minput_ids[:, cur_context],\n\u001b[1;32m    190\u001b[0m             attention_mask\u001b[39m=\u001b[39;49mattention_mask[:, cur_context],\n\u001b[1;32m    191\u001b[0m             past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    192\u001b[0m             use_cache\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    193\u001b[0m         )\n\u001b[1;32m    194\u001b[0m     \u001b[39mif\u001b[39;00m(intervention_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    195\u001b[0m         intervention_function \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1044\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[39m    `-100` are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1044\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1045\u001b[0m     input_ids,\n\u001b[1;32m   1046\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1047\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1048\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1049\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1050\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1051\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1052\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1053\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1054\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1055\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1056\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1057\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1058\u001b[0m )\n\u001b[1;32m   1059\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1061\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:887\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    877\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    878\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    879\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    884\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    885\u001b[0m     )\n\u001b[1;32m    886\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 887\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    888\u001b[0m         hidden_states,\n\u001b[1;32m    889\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    890\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    891\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    892\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    893\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    894\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    895\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    896\u001b[0m     )\n\u001b[1;32m    898\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    899\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/torch/nn/modules/module.py:1120\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1118\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> 1120\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1121\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1122\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:395\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    393\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    394\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 395\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    396\u001b[0m     hidden_states,\n\u001b[1;32m    397\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    398\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    399\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    400\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    401\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    402\u001b[0m )\n\u001b[1;32m    403\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    404\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rome/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:325\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[39mif\u001b[39;00m layer_past \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     past_key, past_value \u001b[39m=\u001b[39m layer_past\n\u001b[0;32m--> 325\u001b[0m     key \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((past_key, key), dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m    326\u001b[0m     value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((past_value, value), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    328\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "testing_dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "target = []\n",
    "predict = []\n",
    "\n",
    "for reviews, sentiment in tqdm(testing_dataloader):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        list(reviews),\n",
    "        padding = True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(next(model.parameters()).device)\n",
    "\n",
    "    if(tokenized_inputs['input_ids'].shape[1] > max_token_per_comment):\n",
    "        # print(f\"BLOCKED ==> {tokenized_inputs['input_ids'].shape[1]}\")\n",
    "        continue\n",
    "\n",
    "    last_token_inds = tokenized_inputs[\"attention_mask\"].sum(dim=1)\n",
    "    max_out_len = max(last_token_inds).item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        txt, ret_dict = model_utils.generate_fast(\n",
    "            model, tokenizer,\n",
    "            list(reviews),\n",
    "            argmax_greedy = True,\n",
    "            max_out_len= max_out_len + prefix_size + 3,\n",
    "            # debug=True,\n",
    "            get_answer_tokens=True,\n",
    "\n",
    "            light_weight_tuning= prefix_embeddings, algo = \"prefix\"\n",
    "        )\n",
    "\n",
    "    for t, p in zip(list(sentiment), ret_dict['answer']):\n",
    "        target.append(t)\n",
    "        predict.append(p['top_token'])\n",
    "\n",
    "    # print(txt, ret_dict['answer'])\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n",
      "19 20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(target, predict).ravel()\n",
    "\n",
    "print(tp, fp)\n",
    "print(fn, tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9198717948717949"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity = tp/(tp + fn)\n",
    "specificity = tn/(tn + fp)\n",
    "balanced_acc = (sensitivity + specificity)/2\n",
    "\n",
    "balanced_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' positive': 687, ' negative': 713}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3835239043501baad7b502b0573c70a3454f6c2753902e68361683a11a30d10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
